# -*- coding: utf-8 -*-
"""Breast Cancer XG-Boost

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/135aAWTG4llhQthH9xJ1xVqFEcCV94tGz

# 1. Cargando Dataset y librerías
"""

import sklearn
assert sklearn.__version__>='0.20'

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings(action='ignore', message='^internal gelsd')

df = pd.read_csv('data.csv')

df.head()

# Validación
df.shape

# Vista general del dataset
df.info()

# Se observa: variables numéricas excepto el target(diagnosis)
# Variables como id y unamed no entran en el análisis

"""## Análisis del Target

"""

sns.countplot(x='diagnosis', data=df, palette ='hls')

# No se observa un desvalanceo de datos
# Debe pasarse a variable numérica para ser analizada

# Conteo de valores de la variable objetivo
df['diagnosis'].value_counts()

# B: Benigno, M: Maligno

# Realizando el cambio B->1, M->0
df['diagnosis'] = df['diagnosis'].replace({'M':0, 'B':1})

# Validando el cambio
df['diagnosis'].value_counts()

# Porcentaje de tumores benignos
df.diagnosis.mean()

# Identificación de variables
features = list(set(df.columns.tolist())-set(['Unnamed: 32', 'diagnosis', 'id']))
features

# Determinando matrices de datos
X = df[features]
y = df.diagnosis # Variable objetivo

"""# 2. Muestreo"""

# Listas para almacenar las métricas de cada una de las 30 corridas
recall_scores = []
specificity_scores = []
auc_scores = []
precision_scores = []
f1_scores = []
accuracy_scores = []
gini_scores = []


# Muestreo de la data
from sklearn.model_selection import train_test_split

for i in range(30):
  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                      stratify = y,
                                                      train_size = 0.6,
                                                      random_state = i)

  # Partiendo el 40% restante entre test y watch
  X_watch, X_test, y_watch, y_test = train_test_split(X_test, y_test,
                                                      stratify = y_test,
                                                      train_size = 0.5,
                                                      random_state = i)

  # La data de watch será para controlar el overfitting.

  # Para realizar el modelo de XGBoost necesitamos cambiar el formato de pd as DMatrix
  import numpy as np
  import xgboost as xgb

  # Cambiando formato a .DMatrix()
  dtrain = xgb.DMatrix(X_train, label = y_train)
  dwatch = xgb.DMatrix(X_watch, label = y_watch)
  dtest = xgb.DMatrix(X_test, label = y_test)

  # Definiendo parámetros
  param ={'objective': 'binary:logistic', # Tipo de aprendizaje
            'max_depth': 8, # Profundidad del árbol
            'learning_rate': 0.05,
            'min_data_leaf': 0.05, # Min data en cada hoja
            'grow_policy': 'lossguide', #Aprendizaje por error
            'eval_metric': 'auc', # Para evaluar el modelo
            'seed': 123
            }
  num_round = 500
  evallist = [(dtrain,'train'), (dwatch,'watchlist')]

  # Entrenando el algoritmo

  import time

  start_time = time.time()
  xgBoost = xgb.train(param, dtrain, num_round, evallist,
                      early_stopping_rounds=5)

  print('---%s seconds' % ((time.time()-start_time)))

  # early_stopping_rounds=5 : Al quinto sobreajuste encontrado se parará el\
  # entrenamiento

  # Importancia de las variables con .get_score()
  importance = xgBoost.get_score(importance_type = 'total_gain')
  importance

  # total_gain: Sum tot de las ganancias de cada variable

  # Ordenando
  pdVarImp = pd.DataFrame({'Feature': list(importance.keys()),
                          'Importance': list(importance.values())}).sort_values('Importance', ascending = False)

  pdVarImp['Orden'] = np.arange(len(pdVarImp)) + 1
  pdVarImp

  # Suma de los valores de ganancia
  plt.plot(pdVarImp.Orden, pdVarImp.Importance.cumsum(axis=0))

  # Reescalando
  pdVarImp['porc_gain'] = pdVarImp.Importance.apply(lambda x: x/pdVarImp.Importance.sum())
  plt.plot(pdVarImp.Orden, pdVarImp.porc_gain.cumsum(axis=0))

  pdVarImp['porc_gain_acum'] = pdVarImp.porc_gain.cumsum(axis = 0)

  # Usando el modelo para predecir
  X_train['probability'] = xgBoost.predict(xgb.DMatrix(X_train[features]))
  X_test['probability'] = xgBoost.predict(xgb.DMatrix(X_test[features]))

  X_train['prediction'] = X_train.probability.apply(lambda x:1 if x > 0.5 else 0)
  X_test['prediction'] = X_test.probability.apply(lambda x:1 if x > 0.5 else 0)

  #Performance
  from sklearn.metrics import *

  # Matriz de confusión
  tn, fp, fn, tp = confusion_matrix(y_test, X_test.prediction).ravel()
  specificity = tn / (tn + fp)
  recall = recall_score(y_test, X_test.prediction)
  auc = roc_auc_score(y_test, X_test.probability)
  precision = precision_score(y_test, X_test.prediction)
  f1 = f1_score(y_test, X_test.prediction)
  accuracy = accuracy_score(y_test, X_test.prediction)
  gini = 2 * auc - 1

  # Almacenando las métricas en las listas correspondientes
  recall_scores.append(recall)
  specificity_scores.append(specificity)
  auc_scores.append(auc)
  precision_scores.append(precision)
  f1_scores.append(f1)
  accuracy_scores.append(accuracy)
  gini_scores.append(gini)

specificity_scores

# Valores estadísticos Recall
df_recall = pd.DataFrame(recall_scores, columns=['valores'])
val_est_recall = df_recall['valores'].describe()
print(val_est_recall)

mediana = df_recall['valores'].median()
print("Mediana:", mediana)

# Valores estadísticos AUC

df_auc = pd.DataFrame(auc_scores, columns=['valores'])
val_est_auc = df_auc['valores'].describe()
print(val_est_auc)

mediana = df_auc['valores'].median()
print("Mediana:", mediana)

# Valores estadísticos Precision

df_precision = pd.DataFrame(precision_scores, columns=['valores'])
val_est_precision = df_precision['valores'].describe()
print(val_est_precision)

mediana = df_precision['valores'].median()
print("Mediana:", mediana)

# Valores estadísticos F1

df_f1 = pd.DataFrame(f1_scores, columns=['valores'])
val_est_f1 = df_f1['valores'].describe()
print(val_est_f1)

mediana = df_f1['valores'].median()
print("Mediana:", mediana)

# Valores estadísticos accuracy

df_accuracy = pd.DataFrame(accuracy_scores, columns=['valores'])
val_est_accuracy = df_accuracy['valores'].describe()
print(val_est_accuracy)

mediana = df_accuracy['valores'].median()
print("Mediana:", mediana)

# Valores estadísticos gini

df_gini = pd.DataFrame(gini_scores, columns=['valores'])
val_est_gini = df_gini['valores'].describe()
print(val_est_gini)

mediana = df_gini['valores'].median()
print("Mediana:", mediana)



# Specificity
tn, fp, fn, tp = confusion_matrix(y_test, X_test.prediction).ravel()

specificity = tn/(tn+fp)
print('Test Specificity: ', specificity)

